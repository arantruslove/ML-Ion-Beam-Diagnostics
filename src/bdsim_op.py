"""
Script that carries out Bayesian optimisation of filter thicknesses for synthetic
images generated by BDSIM.
"""

import numpy as np
import optuna
import pickle
import os
import torch

from bdsim.generation import BDSIMGenerator
from machine_learning.pytorch_trial import ml_trial
import utils

# Get the current directory path
current_file_path = os.path.abspath(__file__)
current_dir = os.path.dirname(current_file_path)

best_images_and_labels_path = (
    f"{current_dir}/../output/synthetic_images/bdsim_op_filters_images.pickle"
)
best_model_path = f"{current_dir}/../output/nn_models/op-filters-model.pth"
study_db_path = f"{current_dir}/../output/optuna_studies/bdsim_op_filters.db"
study_name = "bdsim_op_filters"

TIMEOUT = 20 * 60 * 60
N_IMAGES = 1000
N_WORKERS = 100

# Constants
JOB_ID = "optuna"
N_RANGE_PROTON = (1e7, 1e10)
E_MAX_RANGE_PROTON = (0.1, 5)
T_P_RANGE_PROTON = (0.05, 2)
N_RANGE_ELECTRON = (1e7, 1e10)
T_P_RANGE_ELECTRON = (0.05, 2)
N_MACROPARTICLES = 25000  # 25000 electrons and 25000 protons
FILTER_SIZE = 0.001
FILTER_CENTRE = (0, 0, 0.05)
N_FILTERS = 9
SC_THICKNESS = 20e-6
PIXEL_NO = 30
CLEAR_FILES = False


def generate_data(trial):
    """
    Optimisation of filter thicknesses.
    """
    LOWER_BND = 1e-7
    # Trialing a set of unique filter thicknesses that are optimised separately
    start = trial.suggest_float("thickest", 1e-7, 1e-2, log=True)
    diffs = np.array(
        [
            trial.suggest_float("diff_1", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_2", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_3", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_4", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_5", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_6", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_7", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_8", 1e-7, 1e-2, log=True),
        ]
    )

    filter_array = utils.diffs_to_vals(start, diffs)
    filter_array = utils.set_lower_bnd(filter_array, LOWER_BND)
    trial.set_user_attr("filter_thicknesses", filter_array.tolist())

    # Defining class object and generating data
    dataset = BDSIMGenerator(
        E_MAX_RANGE_PROTON,
        T_P_RANGE_PROTON,
        N_RANGE_PROTON,
        JOB_ID,
        N_MACROPARTICLES,
        N_IMAGES,
        N_WORKERS,
        FILTER_SIZE,
        FILTER_CENTRE,
        filter_array,
        SC_THICKNESS,
        PIXEL_NO,
        T_P_RANGE_ELECTRON,
        N_RANGE_ELECTRON,
        CLEAR_FILES,
    )
    images_and_labels = dataset.gen_many_parallel()

    return images_and_labels


def objective(trial, best_attributes):
    """
    Generates a set of synthetic images and labels with filter parameters selected by
    Optuna. Trains a neural network on this dataset of synthetic images and returns the
    validation loss which is to be minimised by Optuna's algorithm.
    """

    # Tracking best trial attributes
    best_loss = None

    # Generating data
    images_and_labels = generate_data(trial)

    images = images_and_labels["images"]

    # Normalising images and labels
    images_scaler = utils.DynamicMinMaxScaler()
    labels_scaler = utils.DynamicMinMaxScaler()
    logged_labels = np.array(images_and_labels["labels"])[:, :3]
    logged_labels[:, 2] = np.log10(logged_labels[:, 2])
    images = images_scaler.fit_transform(np.array(images))
    labels = labels_scaler.fit_transform(np.array(logged_labels))

    # Splitting into training and validation data
    N_TRAIN = (len(images) * 3) // 4
    train_images = images[:N_TRAIN]
    train_labels = labels[:N_TRAIN]
    test_images = images[N_TRAIN:]
    test_labels = labels[N_TRAIN:]

    # Training Model
    val_loss, model = ml_trial(train_images, train_labels, test_images, test_labels)

    # Updating images, labels, and models if this is the new best trial
    if best_loss is None or val_loss < best_loss:
        best_loss = val_loss
        best_attributes["images_and_labels"] = images_and_labels
        best_attributes["model"] = model

    return val_loss


def main():
    utils.create_output_dirs()

    # Deleting any existing dbs with the same name
    if os.path.exists(study_db_path):
        os.remove(study_db_path)

    study = optuna.create_study(
        direction="minimize",
        study_name=study_name,
        storage=f"sqlite:///{study_db_path}",
    )

    # Specifying initial filter parameters to guide Optuna's search
    study.enqueue_trial({"thickest": 1e-2, "diff_1": 9e-3, "diff_2": 9e-4, "diff_3": 5e-5, "diff_4": 4e-5, "diff_5": 5e-6, "diff_6": 4e-6, "diff_7": 5e-7, "diff_8": 4e-7})

    best_attributes = {"images_and_labels": None, "model": None}
    study.optimize(lambda trial: objective(trial, best_attributes), timeout=TIMEOUT)

    # Saving the best images and labels
    with open(best_images_and_labels_path, "wb") as file:
        pickle.dump(best_attributes["images_and_labels"], file)

    # Saving the best model
    torch.save(best_attributes["model"].state_dict(), best_model_path)


if __name__ == "__main__":
    main()
