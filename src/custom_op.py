"""
Script that carries out Bayesian optimisation of filter thicknesses for synthetic
images generated by the custom implementation.
"""

import os
import numpy as np
import optuna
import pickle

from machine_learning.keras_trial import ml_trial
import custom.filter as fil
import custom.generation as dg
import utils

# Get the current directory path
current_file_path = os.path.abspath(__file__)
current_dir = os.path.dirname(current_file_path)

best_images_and_labels_path = (
    f"{current_dir}/../output/synthetic_images/custom_op_filters.pickle"
)
best_model_path = f"{current_dir}/../output/nn_models/custom_op_filters_model.keras"
study_db_path = f"{current_dir}/../output/optuna_studies/custom_op_filters.db"
study_name = "op-filters"


N_IMAGES = 5000
N_WORKERS = 100
TIMEOUT = 20 * 60 * 60
ADD_ELECTRONS = True

# Image properties
N_FILTERS = 9
E_MAX_BNDS = (0.1, 5)
T_P_BNDS = (0.05, 2)
N_PARTICLE_BNDS = (10e7, 10e10)
N_MACROPARTICLES = int(1e5)


def generate_data(trial):
    """Optimisation of filter attributes."""
    # Trialing a set of unique filter thicknesses
    LOWER_BND = 1e-7
    # Trialing a set of unique filter thicknesses that are optimised separately
    start = trial.suggest_float("thickest", 1e-7, 1e-2, log=True)
    diffs = np.array(
        [
            trial.suggest_float("diff_1", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_2", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_3", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_4", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_5", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_6", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_7", 1e-7, 1e-2, log=True),
            trial.suggest_float("diff_8", 1e-7, 1e-2, log=True),
        ]
    )

    filter_array = utils.diffs_to_vals(start, diffs)
    filter_array = utils.set_lower_bnd(filter_array, LOWER_BND)
    trial.set_user_attr("filter_thicknesses", filter_array.tolist())

    # Filter
    base_unit = [
        [filter_array[0], filter_array[1], filter_array[2]],
        [filter_array[3], filter_array[4], filter_array[5]],
        [filter_array[6], filter_array[7], filter_array[8]],
    ]
    filter = fil.Filter(base_unit, 10, (1, 1))

    # Generating the data
    images_and_labels = dg.gen_many_parallel(
        E_MAX_BNDS,
        T_P_BNDS,
        N_PARTICLE_BNDS,
        N_MACROPARTICLES,
        np.array(filter.filter),
        filter.map,
        N_IMAGES,
        N_WORKERS,
        add_electrons=ADD_ELECTRONS,
    )
    return images_and_labels


def objective(trial, best_attributes):

    # Tracking best trial attributes
    best_loss = None

    # Generating data
    images_and_labels = generate_data(trial)

    # Normalising images and labels
    images_scaler = utils.DynamicMinMaxScaler()
    labels_scaler = utils.DynamicMinMaxScaler()

    logged_labels = np.array(images_and_labels["labels"])[:, :3]  # Only proton params
    logged_labels[:, 2] = np.log10(logged_labels[:, 2])  # Logging N0 param

    images = images_scaler.fit_transform(np.array(images_and_labels["images"]))
    labels = labels_scaler.fit_transform(np.array(logged_labels))

    # Splitting into training and validation data
    N_TRAIN = (len(images) * 3) // 4
    train_images = images[:N_TRAIN]
    train_labels = labels[:N_TRAIN]
    test_images = images[N_TRAIN:]
    test_labels = labels[N_TRAIN:]

    # Training Model
    BATCH_SIZE = 32
    MAX_EPOCHS = 1000
    PATIENCE = 15
    loss, model = ml_trial(
        train_images,
        train_labels,
        test_images,
        test_labels,
        BATCH_SIZE,
        MAX_EPOCHS,
        patience=PATIENCE,
    )

    # Updating if this is a new best trial
    if best_loss is None or loss < best_loss:
        best_loss = loss
        best_attributes["images_and_labels"] = images_and_labels
        best_attributes["model"] = model

    return loss


def main():
    utils.create_output_dirs()

    # Deleting any existing dbs with the same name
    if os.path.exists(study_db_path):
        os.remove(study_db_path)

    study = optuna.create_study(
        direction="minimize",
        study_name=study_name,
        storage=f"sqlite:///{study_db_path}",
    )

    # Specifying initial filter parameters to guide Optuna's search
    study.enqueue_trial(
        {
            "thickest": 1e-2,
            "diff_1": 9e-3,
            "diff_2": 9e-4,
            "diff_3": 5e-5,
            "diff_4": 4e-5,
            "diff_5": 5e-6,
            "diff_6": 4e-6,
            "diff_7": 5e-7,
            "diff_8": 4e-7,
        }
    )

    best_attributes = {"images_and_labels": None, "model": None}
    study.optimize(lambda trial: objective(trial, best_attributes), timeout=TIMEOUT)

    # Saving the best images and labels
    with open(best_images_and_labels_path, "wb") as file:
        pickle.dump(best_attributes["images_and_labels"], file)

    # Saving the best model
    best_attributes["model"].save(best_model_path)


if __name__ == "__main__":
    main()
